"""CLI pour interroger Nina en local.

Assume que LocalAI ou Ollama tourne en local et expose une API OpenAI
compatible (par dÃ©faut http://localhost:8080/v1).

Variables d'environnement utilisables :
  OPENAI_API_BASE  URL de l'endpoint (dÃ©faut : http://localhost:8080/v1)
  OPENAI_API_KEY   ClÃ© fictive (LocalAI n'en vÃ©rifie pas la valeur)
  OPENAI_MODEL     Nom du modÃ¨le (ex. "mistral-7b")
"""
import os
import sys
from dotenv import load_dotenv

# Charger les variables d'environnement depuis le fichier .env
load_dotenv()

# Ajouter le rÃ©pertoire parent au path pour trouver les modules
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

from app.orchestrator import Orchestrator
from agents.agent_openrouter import AgentOpenRouter
from tools.vector_db import VectorDB

def classify_intent(query: str, llm: AgentOpenRouter) -> str:
    """Classifie l'intention de l'utilisateur en 'conversation' ou 'tÃ¢che'."""
    prompt = f"""
    Analysez la demande de l'utilisateur suivante.
    Est-ce une simple question, une salutation ou une phrase de conversation courante ? Ou est-ce une tÃ¢che complexe qui nÃ©cessite de faire une recherche web ou d'interagir avec des fichiers ?
    RÃ©pondez uniquement par le mot "conversation" ou "tÃ¢che".

    Demande de l'utilisateur: "{query}"
    """
    messages = [{"role": "user", "content": prompt}]
    # On utilise le modÃ¨le le plus rapide et le moins cher pour cette classification
    response = llm.invoke("anthropic/claude-3-haiku", messages, temperature=0.0, max_tokens=10)
    
    # Nettoyage de la rÃ©ponse pour Ãªtre sÃ»r
    if "tÃ¢che" in response.lower():
        return "tÃ¢che"
    return "conversation"

def main():
    """
    Lance une conversation interactive avec Nina, propulsÃ©e par l'orchestrateur ReAct.
    """
    print("Initialisation de Nina...")
    
    # RÃ©cupÃ©rer la clÃ© API et initialiser l'orchestrateur
    api_key = os.getenv("OPENROUTER_API_KEY")
    if not api_key or "sk-or-v1-..." in api_key:
        print("\nERREUR: ClÃ© API OpenRouter non configurÃ©e.")
        print("Veuillez crÃ©er un fichier .env Ã  la racine du projet avec la ligne :")
        print('OPENROUTER_API_KEY="votre_cle"')
        return
        
    try:
        orchestrator = Orchestrator(openrouter_api_key=api_key)
        conversational_agent = AgentOpenRouter(api_key)
        vector_db = VectorDB() # Initialisation de la mÃ©moire vectorielle
    except Exception as e:
        print(f"Erreur lors de l'initialisation des composants : {e}")
        return

    print("\n---")
    print("ðŸ¤– Nina est prÃªte. Je suis Ã  votre Ã©coute.")
    print("   Tapez 'exit' ou 'quit' pour terminer la conversation.")
    print("---\n")

    while True:
        try:
            user_input = input("Vous > ")
        except (KeyboardInterrupt, EOFError):
            print("\n\nAu revoir !\n")
            break

        if user_input.lower().strip() in {"exit", "quit"}:
            print("\nAu revoir !\n")
            break
        
        if not user_input.strip():
            continue

        # --- Ã‰TAGE 1: INTERROGATION DE LA MÃ‰MOIRE ---
        print("Nina cherche dans sa mÃ©moire...")
        # On cherche les 2 documents les plus pertinents
        memory_results = vector_db.similarity_search(user_input, top_k=2) 
        
        # On garde seulement les rÃ©sultats non-vides
        meaningful_results = [res['text'] for res in memory_results if res.get('text')]

        if meaningful_results:
            print("Nina a trouvÃ© des informations pertinentes dans sa mÃ©moire.")
            context = f"Contexte trouvÃ© dans ma mÃ©moire :\n--- {' '.join(meaningful_results)}\n---\n\nEn te basant UNIQUEMENT sur ce contexte, rÃ©ponds Ã  la question de l'utilisateur : '{user_input}'"
            messages = [{"role": "user", "content": context}]
            response = conversational_agent.invoke("anthropic/claude-3-haiku", messages)
            print("\nNina >", response, "\n")
            continue # On passe Ã  la prochaine itÃ©ration de la boucle

        # --- Ã‰TAGE 2: ROUTEUR D'INTENTION (si la mÃ©moire est vide) ---
        print("Rien dans la mÃ©moire, Nina analyse la demande...")
        intent = classify_intent(user_input, conversational_agent)
        print(f"(Intention dÃ©tectÃ©e: {intent})")

        if intent == "tÃ¢che":
            print("Nina rÃ©flÃ©chit Ã  une tÃ¢che complexe...")
            try:
                final_response = orchestrator.run(user_input)
                print("\nNina >", final_response, "\n")
            except Exception as e:
                print(f"\nUne erreur est survenue : {e}\n")
        else: # 'conversation'
            print("Nina prÃ©pare une rÃ©ponse...")
            try:
                # Appel direct pour une rÃ©ponse conversationnelle
                messages = [{"role": "user", "content": user_input}]
                response = conversational_agent.invoke("anthropic/claude-3-haiku", messages)
                print("\nNina >", response, "\n")
            except Exception as e:
                print(f"\nUne erreur est survenue : {e}\n")

if __name__ == "__main__":
    main() 