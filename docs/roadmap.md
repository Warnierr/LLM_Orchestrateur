# Roadmap du projet Nina

## Statut actuel

- ğŸ“¦ Phases 1 Ã  5 complÃ©tÃ©es :
  - Fondations, CLI vLLM, mÃ©moire vectorielle, orchestrateur multi-agents, intÃ©gration de vLLM (>=0.8.5) avec Function Calling de base.

## Prochaines Ã©tapes

1. ğŸ—‚ï¸ HiÃ©rarchisation de la mÃ©moire (MemoryManager Ã  trois couches)
   - MÃ©moire de travail (fenÃªtre glissante des 10â€“20 derniers messages)
   - MÃ©moire sÃ©mantique (vectorielle / RAG)
   - MÃ©moire condensÃ©e (rÃ©sumÃ©s pÃ©riodiques via agent Summarizer + table SQL `summaries`)
2. ğŸ› ï¸ MÃ©moire personnalisÃ©e (SQL + Qdrant)
   - `tools/sql_db.py` (SQLAlchemy) + `tools/vector_db.py` (Qdrant)
   - Configuration `DATABASE_URL` (SQLite pour POC, PostgreSQL en production)
3. ğŸ¤– Fiabilisation de Nina LLM
   - ChaÃ®ne de raisonnement dÃ©taillÃ©e (chain-of-thought)
   - Gestion robuste des appels de fonctions (fallback local/cloud)
4. ğŸ” AmÃ©lioration des embeddings & re-ranking
   - Remplacer l'embedder maison par un modÃ¨le HF (`SentenceTransformers`, DPRâ€¦)
   - Cross-encoder (`cross-encoder/ms-marco-MiniLM-L-6-v2`) pour filtrage des passages
5. ğŸ“š IntÃ©gration Agent Wikipedia via LangChain
   - Utiliser `WikipediaAPIWrapper` ou `WikipediaQueryRun` de LangChain pour interroger l'API Wikipedia
   - CrÃ©er `agents/agent_wikipedia.py` avec la classe `AgentWikipedia`
   - Ajouter `agent_wikipedia` dans `configs/crew_config.yaml` et exposer via Function Calling dans `AgentNina`
6. ğŸ¤– Ajout Agent Grok (Salesforce)
   - CrÃ©er `agents/agent_grok.py` avec un endpoint REST et clÃ© API
   - Exposer `AgentGrok` dans `configs/crew_config.yaml` sous `agents:`
   - Ajouter la fonction `grok_query` dans `AgentNina.think_and_respond` pour les dÃ©clencheurs "grok", "Grok AI"
7. â™»ï¸ Auto-critique & apprentissage continu
   - Boucle d'Ã©valuation post-rÃ©ponse, feedback utilisateur, ajustement de prompts
   - RÃ©entrainement incrÃ©mental selon retours
8. ğŸ“Š Knowledge Graph d'entitÃ©s
   - Extraction NER (SpaCy, HF) et relations
   - Stockage Neo4j / RDF + requÃªtes factuelles (ex. "Quels projets AI ai-je mentionnÃ©s ?")
9. ğŸ¼ Multi-modalitÃ©
   - IntÃ©gration STT (Whisper) et TTS (Coqui, pyttsx3â€¦)
   - Support audio & images
10. âš™ï¸ Orchestrateur avancÃ©
    - ExpÃ©rimenter CrewAI vs LangChain pour workflows dynamiques
    - Uniformiser appels de fonctions, tracing & monitoring
11. ğŸ”’ SÃ©curitÃ© & actions systÃ¨me
    - Agent Shell contrÃ´lÃ© (sandbox)
    - Gouvernance des permissions
12. ğŸ“ˆ ScalabilitÃ© & production
    - Qdrant en Docker / FAISS sur disque pour indices persistants
    - DÃ©ployer PostgreSQL ou Redis pour la partie SQL
    - CI/CD, tests d'intÃ©gration, monitoring (logs, mÃ©triques)
    - Containerisation (Docker + Kubernetes), packaging pip
13. ğŸ¤ Modules audio (STT Whisper, TTS)
14. ğŸŒ Interface web & observabilitÃ© (Streamlit, logging structurÃ©)
15. âœ… Tests & CI/CD (couverture tests, GitHub Actions)
16. ğŸ“¦ DÃ©ploiement & packaging (Docker, pip)
17. ğŸ“š Documentation & push Git (README, docs/, guides)
18. ğŸ” RÃ©sultats du test d'intelligence gÃ©nÃ©rale

## AmÃ©liorations futures du raisonnement LLM

10. **IntÃ©gration de LLMs de pointe** : Tester et intÃ©grer des modÃ¨les plus performants supportÃ©s par la configuration matÃ©rielle actuelle.
    - **Option A (QualitÃ© Maximale)** : `Mistral-Mixtral-8x7B-Instruct` pour une capacitÃ© de raisonnement et de synthÃ¨se supÃ©rieure.
    - **Option B (RÃ©activitÃ© Maximale)** : `Meta-Llama-3-8B-Instruct` pour un Ã©quilibre parfait entre performance et vitesse.
11. **ChaÃ®ne de PensÃ©e (Chain-of-Thought)** : Modifier le prompt pour demander au LLM d'expliciter son raisonnement Ã©tape par Ã©tape avant de donner la rÃ©ponse finale, afin d'amÃ©liorer la qualitÃ© et la fiabilitÃ© des rÃ©ponses complexes.
12. **Apprentissage par l'exemple (Few-shot Prompting)** : Pour les tÃ¢ches de raisonnement, inclure dans le prompt un ou deux exemples de questions/rÃ©ponses correctes pour "montrer" au LLM le format et la logique attendus.
13. **MontÃ©e en gamme du modÃ¨le (Futur)** : Pour des capacitÃ©s de raisonnement au-delÃ  des standards actuels, envisager de passer Ã  de futurs modÃ¨les nÃ©cessitant une infrastructure de type serveur.

## Ã‰tape suivante immÃ©diate

- ImplÃ©menter l'Ã©tape 5 : complÃ©ter `agents/agent_llm_local.py` et `app/cli.py` pour que Nina LLM utilise pleinement l'Orchestrateur via Function Calling. 