#!/usr/bin/env python3
"""
ðŸ¤– Agent Nina - Version corrigÃ©e

L'agent principal qui orchestre tous les autres agents de maniÃ¨re intelligente.
"""

import sys
import os
import time
from enum import Enum
from dataclasses import dataclass
from typing import List, Dict, Any, Optional

# Ajout du chemin racine pour les imports
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

from agents.agent_chercheur_v3 import AgentChercheurV3
from agents.agent_analyste import AgentAnalyste, AgentApprentissage
from agents.agent_redacteur import AgentRedacteur
from agents.agent_planificateur import AgentPlanificateur
from agents.agent_news import AgentNews
from tools.vector_db import VectorDB

class TaskType(Enum):
    """Types de tÃ¢ches que Nina peut traiter."""
    RECHERCHE = "recherche"
    ACTUALITES = "actualites"
    ANALYSE = "analyse"
    PLANIFICATION = "planification"
    CONVERSATION = "conversation"
    APPRENTISSAGE = "apprentissage"

class TaskComplexity(Enum):
    """ComplexitÃ© des tÃ¢ches."""
    SIMPLE = "simple"      # 1-2 agents
    MODERATE = "moderate"  # 2-4 agents
    COMPLEX = "complex"    # 4+ agents

@dataclass
class TaskPlan:
    """Plan d'exÃ©cution d'une tÃ¢che."""
    task_type: TaskType
    complexity: TaskComplexity
    agents_needed: List[str]
    estimated_time: float
    priority: int
    reasoning: str

class AgentNina:
    """
    ðŸ¤– Agent Nina - Le cerveau de l'orchestrateur
    
    Nina analyse les requÃªtes, planifie l'exÃ©cution et coordonne
    tous les agents spÃ©cialisÃ©s pour fournir des rÃ©ponses intelligentes.
    """
    
    def __init__(self):
        # Agents spÃ©cialisÃ©s
        self.chercheur = AgentChercheurV3()
        self.analyste = AgentAnalyste()
        self.apprenant = AgentApprentissage()
        self.redacteur = AgentRedacteur()
        self.planificateur = AgentPlanificateur()
        self.news_agent = AgentNews()
        
        # Classe ObjectifAgent simulÃ©e
        class ObjectifAgent:
            def planifier(self): pass
        self.objectif = ObjectifAgent()
        
        # Base de donnÃ©es vectorielle
        self.vectordb = VectorDB()
        
        # Historique et statistiques
        self.conversation_history = []
        self.task_stats = {
            "total_tasks": 0,
            "successful_tasks": 0,
            "avg_response_time": 0.0,
            "agent_usage": {},
            "user_satisfaction": []
        }
        # Charger le profil utilisateur depuis crew_config.yaml
        try:
            import yaml  # type: ignore
            cfg_path = os.path.join(os.path.dirname(os.path.dirname(os.path.abspath(__file__))), 'configs', 'crew_config.yaml')
            with open(cfg_path, 'r', encoding='utf-8') as f:
                cfg = yaml.safe_load(f) or {}
            self.user_profile = cfg.get('parameters', {})
        except Exception:
            self.user_profile = {}
        # Initialiser l'agent LLM local si configurÃ©
        self.local_llm = None
        if self.user_profile.get('use_local_llm'):
            try:
                model_path = self.user_profile.get('local_llm_model')
                if not isinstance(model_path, str) or not model_path:
                    raise ValueError("ClÃ© 'local_llm_model' invalide ou manquante")
                from agents.agent_llm_local import AgentLLMLocal  # type: ignore
                self.local_llm = AgentLLMLocal(model_path)
                print(f"[AgentNina] LLM local chargÃ©: {model_path}")
            except Exception as e:
                print(f"[AgentNina] Impossible d'initialiser LLM local: {e}")
    
    def analyze_request(self, query: str) -> TaskPlan:
        """ðŸ§  Nina analyse la requÃªte et dÃ©cide de la stratÃ©gie optimale."""
        query_lower = query.lower()
        
        # Mots-clÃ©s pour classification (Ã©tendus et amÃ©liorÃ©s)
        news_keywords = [
            "actualitÃ©", "news", "derniÃ¨re", "rÃ©cent", "nouveau", "neuf", 
            "quoi de neuf", "nouveautÃ©", "tendance", "dÃ©veloppement",
            "avancÃ©e", "Ã©volution", "mise Ã  jour", "sortie", "lancement"
        ]
        search_keywords = [
            "recherche", "trouve", "cherche", "information", "qu'est-ce que",
            "comment", "pourquoi", "dÃ©finition", "explication", "dÃ©tail",
            "ressource", "source", "donnÃ©e", "fait", "statistique"
        ]
        analysis_keywords = [
            "analyse", "explique", "compare", "Ã©value", "dÃ©cortique",
            "approfondie", "dÃ©taillÃ©e", "critique", "opinion", "avis",
            "avantages", "inconvÃ©nients", "pour/contre", "implications"
        ]
        planning_keywords = [
            "plan", "Ã©tape", "projet", "organise", "planification",
            "stratÃ©gie", "roadmap", "mÃ©thode", "processus", "guide",
            "comment faire", "tutoriel", "marche Ã  suivre"
        ]
        
        # Classification intelligente avec scoring
        scores = {
            TaskType.ACTUALITES: sum(1 for kw in news_keywords if kw in query_lower),
            TaskType.RECHERCHE: sum(1 for kw in search_keywords if kw in query_lower),
            TaskType.ANALYSE: sum(1 for kw in analysis_keywords if kw in query_lower),
            TaskType.PLANIFICATION: sum(1 for kw in planning_keywords if kw in query_lower)
        }
        
        # Bonus de contexte pour certains sujets
        if any(topic in query_lower for topic in ["ia", "intelligence artificielle", "chatgpt", "llm", "ai"]):
            scores[TaskType.RECHERCHE] += 2  # Favorise la recherche pour les sujets IA
        
        if any(pattern in query_lower for pattern in ["quelles sont", "liste", "types de"]):
            scores[TaskType.RECHERCHE] += 1
            
        if "?" in query and len(query.split()) > 3:
            scores[TaskType.RECHERCHE] += 1  # Questions complexes = recherche
        
        # SÃ©lectionner le type avec le meilleur score
        max_score = max(scores.values()) if scores.values() else 0
        best_type = max(scores, key=lambda k: scores[k]) if max_score > 0 else TaskType.CONVERSATION
        
        # Configuration selon le type dÃ©tectÃ©
        if best_type == TaskType.ACTUALITES:
            task_type = TaskType.ACTUALITES
            agents = ["news", "analyste", "redacteur"]
            complexity = TaskComplexity.MODERATE
            reasoning = f"ActualitÃ©s dÃ©tectÃ©es (score: {scores[best_type]}) - rÃ©cupÃ©ration news + analyse"
            
        elif best_type == TaskType.RECHERCHE:
            task_type = TaskType.RECHERCHE
            agents = ["chercheur", "analyste", "vectordb", "redacteur"]
            complexity = TaskComplexity.MODERATE
            reasoning = f"Recherche dÃ©tectÃ©e (score: {scores[best_type]}) - collecte web + RAG + analyse"
            
        elif best_type == TaskType.ANALYSE:
            task_type = TaskType.ANALYSE
            agents = ["chercheur", "analyste", "apprenant", "redacteur"]
            complexity = TaskComplexity.COMPLEX
            reasoning = f"Analyse dÃ©tectÃ©e (score: {scores[best_type]}) - recherche approfondie + insights"
            
        elif best_type == TaskType.PLANIFICATION:
            task_type = TaskType.PLANIFICATION
            agents = ["planificateur", "objectif", "analyste", "redacteur"]
            complexity = TaskComplexity.COMPLEX
            reasoning = f"Planification dÃ©tectÃ©e (score: {scores[best_type]}) - structuration + Ã©tapes"
            
        else:
            task_type = TaskType.CONVERSATION
            agents = ["vectordb", "apprenant", "redacteur"]
            complexity = TaskComplexity.SIMPLE
            reasoning = "Conversation gÃ©nÃ©rale - mÃ©moire + rÃ©ponse contextuelle"
        
        # Estimation du temps basÃ©e sur la complexitÃ©
        time_estimates = {
            TaskComplexity.SIMPLE: 2.0,
            TaskComplexity.MODERATE: 5.0,
            TaskComplexity.COMPLEX: 8.0
        }
        
        return TaskPlan(
            task_type=task_type,
            complexity=complexity,
            agents_needed=agents,
            estimated_time=time_estimates[complexity],
            priority=1,
            reasoning=reasoning
        )
    
    def _execute_search_task(self, query: str) -> Dict[str, Any]:
        """ExÃ©cution optimisÃ©e pour la recherche."""
        # 1. Recherche dans la mÃ©moire d'abord
        memory_results = self.vectordb.similarity_search(query, top_k=3)
        
        # 2. Recherche web
        web_results = self.chercheur.collect_data("web", query)
        
        # 3. Combinaison et analyse
        all_data = web_results + [r["text"] for r in memory_results]
        insights = self.analyste.analyze_data(all_data) if all_data else {}
        
        # 4. Mise Ã  jour de la mÃ©moire
        if web_results:
            self.vectordb.add_documents(web_results)
        
        return {
            "web_results": web_results,
            "memory_results": memory_results,
            "insights": insights,
            "total_sources": len(all_data)
        }
    
    def think_and_respond(self, query: str) -> str:
        """ðŸ¤– MÃ©thode principale : Nina rÃ©flÃ©chit et rÃ©pond via Function Calling LLM."""
        import json
        # Stocker la requÃªte dans l'historique
        self.conversation_history.append({'query': query})
        # VÃ©rifier si le module openai est disponible
        try:
            import openai
            from openai.error import APIConnectionError  # type: ignore
        except ImportError:
            print("[Nina FunctionCalling] Module openai manquant, fallback pipeline Nina")
            # Fallback Nina pipeline (sans function calling)
            plan = self.analyze_request(query)
            results = self._execute_search_task(query)
            # Apprentissage continu
            if results.get("web_results"):
                self.apprenant.apprendre(results["web_results"])
            if results.get("memory_results"):
                self.apprenant.apprendre([p["text"] for p in results.get("memory_results", [])])
            # GÃ©nÃ©ration de la rÃ©ponse : rapport pour recherche, LLM local pour conversation
            if self.local_llm and plan.task_type == TaskType.CONVERSATION:
                # Construire un prompt pour synthÃ¨se
                system_msg = {"role": "system", "content": "Vous Ãªtes Nina, une assistante IA. Formulez une rÃ©ponse claire et synthÃ©tique Ã  la requÃªte de l'utilisateur."}
                user_msg = {"role": "user", "content": f"RequÃªte: {query}\nDonnÃ©es: {results}"}
                response = self.local_llm.chat([system_msg, user_msg])
            else:
                # TÃ¢ches de recherche et autres utilisent le rapport traditionnel
                response = self._generate_data_rich_response(query, results, plan)
            self.conversation_history[-1]['response'] = response
            return response
        # DÃ©terminer le plan de tÃ¢che pour choisir la stratÃ©gie
        plan = self.analyze_request(query)
        # Construire le prompt initial avec contexte systÃ¨me
        system_msg = {
            "role": "system",
            "content": "Vous Ãªtes Nina, une assistante IA qui orchestre plusieurs agents pour rÃ©pondre aux requÃªtes utilisateurs. Utilisez uniquement les fonctions fournies."
        }
        user_msg = {"role": "user", "content": query}
        # PrÃ©parer la liste de messages, en injectant la mÃ©moire pour les conversations
        messages = [system_msg]
        if plan.task_type == TaskType.CONVERSATION:
            # RÃ©cupÃ©rer les passages mÃ©morisÃ©s les plus pertinents
            memory_results = self.vectordb.similarity_search(query, top_k=3)
            for mem in memory_results:
                messages.append({
                    "role": "system",
                    "content": f"Contexte mÃ©moire : {mem['text']}"
                })
        messages.append(user_msg)
        # Si un LLM local est configurÃ©, l'utiliser en prioritÃ©
        if self.local_llm:
            try:
                response = self.local_llm.chat(messages)
                self.conversation_history[-1]['response'] = response
                # Mise Ã  jour de la mÃ©moire conversationnelle
                from datetime import datetime
                ts = datetime.utcnow().isoformat()
                # Indexer la requÃªte et la rÃ©ponse pour enrichir la mÃ©moire
                self.vectordb.add_documents(
                    [query, response],
                    [{"timestamp": ts}, {"timestamp": ts}]
                )
                return response
            except Exception as e:
                print(f"[AgentNina] Erreur LLM local: {e}, bascule vers OpenAI(FunctionCalling)")
        # Fonction d'appel LLM via OpenAI (LocalAI ou cloud)
        try:
            # Essayer LLM local
            completion = openai.chat.completions.create(  # type: ignore
                model=self.redacteur.model,
                messages=messages,  # type: ignore
                functions=function_defs,  # type: ignore
                function_call="auto",
                temperature=0
            )
        except APIConnectionError as e:
            print(f"[Nina FunctionCalling] Local LLM inaccessible ({e}), bascule vers OpenAI cloud")
            # Bascule vers OpenAI cloud
            openai.base_url = os.getenv("OPENAI_API_BASE", "https://api.openai.com/v1")  # type: ignore
            openai.api_key = os.getenv("OPENAI_API_KEY")
            completion = openai.chat.completions.create(  # type: ignore
                model=self.redacteur.model,
                messages=messages,  # type: ignore
                functions=function_defs,  # type: ignore
                function_call="auto",
                temperature=0
            )
        if not completion or not completion.choices:
            raise RuntimeError("Pas de rÃ©ponse du LLM")
        msg = completion.choices[0].message
        # Si un appel de fonction est demandÃ© par le LLM
        if hasattr(msg, 'function_call') and msg.function_call:
            fname = msg.function_call.name
            args = json.loads(msg.function_call.arguments)
            # Appel de la fonction correspondante
            if fname == "collect_data":
                result = self.chercheur.collect_data(args["source_type"], args["query"])
            elif fname == "analyze_data":
                result = self.analyste.analyze_data(args.get("data", []))
            elif fname == "similarity_search":
                result = self.vectordb.similarity_search(args["query"], top_k=args.get("top_k", 3))
            elif fname == "fetch_ai_news":
                result = self.news_agent.fetch_ai_news()
            else:
                result = {}
            # Ajouter le retour de fonction dans les messages
            messages = [system_msg, user_msg,
                        {"role": "assistant", "content": None, "function_call": msg.function_call},
                        {"role": "function", "name": fname, "content": json.dumps(result)}]
            # 2e appel LLM : synthÃ¨se des rÃ©sultats
            final = openai.chat.completions.create(  # type: ignore
                model=self.redacteur.model,
                messages=messages,  # type: ignore
                temperature=0.2
            )
            response = final.choices[0].message.content or ""
        else:
            # Pas de function_call, retour direct du LLM
            response = msg.content or ""
        # Stocker la rÃ©ponse
        self.conversation_history[-1]['response'] = response
        return response
    
    def _generate_data_rich_response(self, query: str, results: Dict[str, Any], plan: TaskPlan) -> str:
        """âœ¨ Nina gÃ©nÃ¨re un rapport synthÃ©tique via AgentRedacteur."""
        # Fusionner les insights au niveau supÃ©rieur
        insights = results.get("insights", {})
        merged = {}
        merged.update(insights)

        # RAG : recherche de passages similaires
        passages = self.vectordb.similarity_search(query, top_k=3)
        if passages:
            merged["passages_similaires"] = passages
        else:
            # Fallback actualitÃ©s si pas de passages rÃ©cents
            news_items = self.news_agent.fetch_ai_news()
            merged["news"] = news_items

        # GÃ©nÃ©ration du rapport final via AgentRedacteur (en appelant le LLM local si configurÃ©)
        report = self.redacteur.generate_report(merged, history=self.conversation_history, reasoning=plan.reasoning, profile=self.user_profile)
        return report
    
    def get_stats(self) -> Dict[str, Any]:
        """Retourne les statistiques de Nina."""
        return {
            "total_tasks": self.task_stats["total_tasks"],
            "successful_tasks": self.task_stats["successful_tasks"],
            "avg_response_time": self.task_stats["avg_response_time"],
            "agent_usage": self.task_stats["agent_usage"],
            "success_rate": "100.0%",
            "conversation_length": len(self.conversation_history),
            "memory_size": 0
        } 