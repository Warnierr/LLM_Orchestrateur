#!/usr/bin/env python3
"""
ü§ñ Agent Nina - Version corrig√©e

L'agent principal qui orchestre tous les autres agents de mani√®re intelligente.
"""

import sys
import os
import time
import json
from enum import Enum
from dataclasses import dataclass
from typing import List, Dict, Any, Optional

# Ajout du chemin racine pour les imports
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

from agents.agent_chercheur_v3 import AgentChercheurV3
from agents.agent_analyste import AgentAnalyste, AgentApprentissage
from agents.agent_redacteur import AgentRedacteur
from agents.agent_planificateur import AgentPlanificateur
from agents.agent_news import AgentNews
from tools.vector_db import VectorDB
from tools.sql_db import SQLDatabase

class TaskType(Enum):
    """Types de t√¢ches que Nina peut traiter."""
    RECHERCHE = "recherche"
    ACTUALITES = "actualites"
    ANALYSE = "analyse"
    PLANIFICATION = "planification"
    CONVERSATION = "conversation"
    APPRENTISSAGE = "apprentissage"

class TaskComplexity(Enum):
    """Complexit√© des t√¢ches."""
    SIMPLE = "simple"      # 1-2 agents
    MODERATE = "moderate"  # 2-4 agents
    COMPLEX = "complex"    # 4+ agents

@dataclass
class TaskPlan:
    """Plan d'ex√©cution d'une t√¢che."""
    task_type: TaskType
    complexity: TaskComplexity
    agents_needed: List[str]
    estimated_time: float
    priority: int
    reasoning: str

class AgentNina:
    """
    ü§ñ Agent Nina - Le cerveau de l'orchestrateur
    
    Nina analyse les requ√™tes, planifie l'ex√©cution et coordonne
    tous les agents sp√©cialis√©s pour fournir des r√©ponses intelligentes.
    """
    
    def __init__(self):
        # Agents sp√©cialis√©s
        self.chercheur = AgentChercheurV3()
        self.analyste = AgentAnalyste()
        self.apprenant = AgentApprentissage()
        self.redacteur = AgentRedacteur()
        self.planificateur = AgentPlanificateur()
        self.news_agent = AgentNews()
        
        # Classe ObjectifAgent simul√©e
        class ObjectifAgent:
            def planifier(self): pass
        self.objectif = ObjectifAgent()
        
        # Base de donn√©es vectorielle
        self.vectordb = VectorDB()
        
        # Historique et statistiques
        self.conversation_history = []
        self.task_stats = {
            "total_tasks": 0,
            "successful_tasks": 0,
            "avg_response_time": 0.0,
            "agent_usage": {},
            "user_satisfaction": []
        }
        # Charger le profil utilisateur depuis crew_config.yaml
        try:
            import yaml  # type: ignore
            cfg_path = os.path.join(os.path.dirname(os.path.dirname(os.path.abspath(__file__))), 'configs', 'crew_config.yaml')
            with open(cfg_path, 'r', encoding='utf-8') as f:
                cfg = yaml.safe_load(f) or {}
            self.user_profile = cfg.get('parameters', {})
        except Exception:
            self.user_profile = {}
        # Int√©gration SQL pour la m√©moire relationnelle
        db_url = os.getenv('DATABASE_URL', 'sqlite:///data/nina_memory.db')
        self.sql_db = SQLDatabase(db_url)
        # Charger historique de conversation et profil utilisateur depuis la BDD SQL
        self.conversation_history = self.sql_db.load_conversations()
        db_profile = self.sql_db.load_user_profile()
        self.user_profile = {**self.user_profile, **db_profile}
        # Initialiser l'agent LLM local si configur√©
        self.local_llm = None
        if self.user_profile.get('use_local_llm'):
            try:
                model_path = self.user_profile.get('local_llm_model')
                if not isinstance(model_path, str) or not model_path:
                    raise ValueError("Cl√© 'local_llm_model' invalide ou manquante")
                from agents.agent_llm_local import AgentLLMLocal  # type: ignore
                self.local_llm = AgentLLMLocal(model_path)
                print(f"[AgentNina] LLM local charg√©: {model_path}")
            except Exception as e:
                print(f"[AgentNina] Impossible d'initialiser LLM local: {e}")
    
    def analyze_request(self, query: str) -> TaskPlan:
        """üß† Nina analyse la requ√™te et d√©cide de la strat√©gie optimale."""
        query_lower = query.lower()
        
        # Mots-cl√©s pour classification (√©tendus et am√©lior√©s)
        news_keywords = [
            "actualit√©", "news", "derni√®re", "r√©cent", "nouveau", "neuf", 
            "quoi de neuf", "nouveaut√©", "tendance", "d√©veloppement",
            "avanc√©e", "√©volution", "mise √† jour", "sortie", "lancement"
        ]
        search_keywords = [
            "recherche", "trouve", "cherche", "information", "qu'est-ce que",
            "comment", "pourquoi", "d√©finition", "explication", "d√©tail",
            "ressource", "source", "donn√©e", "fait", "statistique"
        ]
        analysis_keywords = [
            "analyse", "explique", "compare", "√©value", "d√©cortique",
            "approfondie", "d√©taill√©e", "critique", "opinion", "avis",
            "avantages", "inconv√©nients", "pour/contre", "implications"
        ]
        planning_keywords = [
            "plan", "√©tape", "projet", "organise", "planification",
            "strat√©gie", "roadmap", "m√©thode", "processus", "guide",
            "comment faire", "tutoriel", "marche √† suivre"
        ]
        
        # Classification intelligente avec scoring
        scores = {
            TaskType.ACTUALITES: sum(1 for kw in news_keywords if kw in query_lower),
            TaskType.RECHERCHE: sum(1 for kw in search_keywords if kw in query_lower),
            TaskType.ANALYSE: sum(1 for kw in analysis_keywords if kw in query_lower),
            TaskType.PLANIFICATION: sum(1 for kw in planning_keywords if kw in query_lower)
        }
        
        # Bonus de contexte pour certains sujets
        if any(topic in query_lower for topic in ["ia", "intelligence artificielle", "chatgpt", "llm", "ai"]):
            scores[TaskType.RECHERCHE] += 2  # Favorise la recherche pour les sujets IA
        
        if any(pattern in query_lower for pattern in ["quelles sont", "liste", "types de"]):
            scores[TaskType.RECHERCHE] += 1
            
        if "?" in query and len(query.split()) > 3:
            scores[TaskType.RECHERCHE] += 1  # Questions complexes = recherche
        
        # S√©lectionner le type avec le meilleur score
        max_score = max(scores.values()) if scores.values() else 0
        best_type = max(scores, key=lambda k: scores[k]) if max_score > 0 else TaskType.CONVERSATION
        
        # Configuration selon le type d√©tect√©
        if best_type == TaskType.ACTUALITES:
            task_type = TaskType.ACTUALITES
            agents = ["news", "analyste", "redacteur"]
            complexity = TaskComplexity.MODERATE
            reasoning = f"Actualit√©s d√©tect√©es (score: {scores[best_type]}) - r√©cup√©ration news + analyse"
            
        elif best_type == TaskType.RECHERCHE:
            task_type = TaskType.RECHERCHE
            agents = ["chercheur", "analyste", "vectordb", "redacteur"]
            complexity = TaskComplexity.MODERATE
            reasoning = f"Recherche d√©tect√©e (score: {scores[best_type]}) - collecte web + RAG + analyse"
            
        elif best_type == TaskType.ANALYSE:
            task_type = TaskType.ANALYSE
            agents = ["chercheur", "analyste", "apprenant", "redacteur"]
            complexity = TaskComplexity.COMPLEX
            reasoning = f"Analyse d√©tect√©e (score: {scores[best_type]}) - recherche approfondie + insights"
            
        elif best_type == TaskType.PLANIFICATION:
            task_type = TaskType.PLANIFICATION
            agents = ["planificateur", "objectif", "analyste", "redacteur"]
            complexity = TaskComplexity.COMPLEX
            reasoning = f"Planification d√©tect√©e (score: {scores[best_type]}) - structuration + √©tapes"
            
        else:
            task_type = TaskType.CONVERSATION
            agents = ["vectordb", "apprenant", "redacteur"]
            complexity = TaskComplexity.SIMPLE
            reasoning = "Conversation g√©n√©rale - m√©moire + r√©ponse contextuelle"
        
        # Estimation du temps bas√©e sur la complexit√©
        time_estimates = {
            TaskComplexity.SIMPLE: 2.0,
            TaskComplexity.MODERATE: 5.0,
            TaskComplexity.COMPLEX: 8.0
        }
        
        return TaskPlan(
            task_type=task_type,
            complexity=complexity,
            agents_needed=agents,
            estimated_time=time_estimates[complexity],
            priority=1,
            reasoning=reasoning
        )
    
    def _execute_search_task(self, query: str) -> Dict[str, Any]:
        """Ex√©cution optimis√©e pour la recherche."""
        # 1. Recherche dans la m√©moire d'abord
        memory_results = self.vectordb.similarity_search(query, top_k=3)
        
        # 2. Recherche web
        web_results = self.chercheur.collect_data("web", query)
        
        # 3. Combinaison et analyse
        all_data = web_results + [r["text"] for r in memory_results]
        insights = self.analyste.analyze_data(all_data) if all_data else {}
        
        # 4. Mise √† jour de la m√©moire
        if web_results:
            self.vectordb.add_documents(web_results)
        
        return {
            "web_results": web_results,
            "memory_results": memory_results,
            "insights": insights,
            "total_sources": len(all_data)
        }
    
    def think_and_respond(self, query: str) -> str:
        """ü§ñ M√©thode principale : Nina r√©fl√©chit et r√©pond via Function Calling LLM."""
        import json
        # Stocker la requ√™te dans l'historique
        self.conversation_history.append({'query': query})
        # V√©rifier si le module openai est disponible
        try:
            import openai
            from openai.error import APIConnectionError  # type: ignore
        except ImportError:
            print("[Nina FunctionCalling] Module openai manquant, fallback pipeline Nina")
            # Fallback Nina pipeline (sans function calling)
            plan = self.analyze_request(query)
            results = self._execute_search_task(query)
            # Apprentissage continu
            if results.get("web_results"):
                self.apprenant.apprendre(results["web_results"])
            if results.get("memory_results"):
                self.apprenant.apprendre([p["text"] for p in results.get("memory_results", [])])
            # G√©n√©ration de la r√©ponse : rapport pour recherche, LLM local pour conversation
            if self.local_llm and plan.task_type == TaskType.CONVERSATION:
                # Construire un prompt pour synth√®se
                system_msg = {"role": "system", "content": "Vous √™tes Nina, une assistante IA. Formulez une r√©ponse claire et synth√©tique √† la requ√™te de l'utilisateur."}
                user_msg = {"role": "user", "content": f"Requ√™te: {query}\nDonn√©es: {results}"}
                response = self.local_llm.chat([system_msg, user_msg])
            else:
                # T√¢ches de recherche et autres utilisent le rapport traditionnel
                response = self._generate_data_rich_response(query, results, plan)
            self.conversation_history[-1]['response'] = response
            # Persister l'interaction dans la BDD SQL
            self.sql_db.save_interaction(query, response)
            return response
        # D√©terminer le plan de t√¢che pour choisir la strat√©gie
        plan = self.analyze_request(query)
        # Construire le prompt initial avec contexte syst√®me
        system_msg = {
            "role": "system",
            "content": (
                "Vous √™tes Nina, une assistante IA qui orchestre plusieurs agents pour r√©pondre aux requ√™tes utilisateurs. "
                "Pour chaque requ√™te, fournissez d'abord votre raisonnement d√©taill√© (chain-of-thought), puis la r√©ponse finale. "
                "Utilisez uniquement les fonctions fournies."
            )
        }
        user_msg = {"role": "user", "content": query}
        # Pr√©parer la liste de messages, en injectant la m√©moire pour les conversations
        messages = [system_msg]
        if plan.task_type == TaskType.CONVERSATION:
            # R√©cup√©rer les passages m√©moris√©s les plus pertinents
            memory_results = self.vectordb.similarity_search(query, top_k=3)
            for mem in memory_results:
                messages.append({
                    "role": "system",
                    "content": f"Contexte m√©moire : {mem['text']}"
                })
        messages.append(user_msg)
        # Si un LLM local est configur√©, l'utiliser en priorit√©
        if self.local_llm:
            try:
                response = self.local_llm.chat(messages)
                self.conversation_history[-1]['response'] = response
                # Mise √† jour de la m√©moire conversationnelle
                from datetime import datetime
                ts = datetime.utcnow().isoformat()
                # Indexer la requ√™te et la r√©ponse pour enrichir la m√©moire
                self.vectordb.add_documents(
                    [query, response],
                    [{"timestamp": ts}, {"timestamp": ts}]
                )
                # Persister l'interaction dans la BDD SQL
                self.sql_db.save_interaction(query, response)
                return response
            except Exception as e:
                print(f"[AgentNina] Erreur LLM local: {e}, bascule vers OpenAI(FunctionCalling)")
        # Fonction d'appel LLM via OpenAI (LocalAI ou cloud)
        try:
            # Essayer LLM local
            completion = openai.chat.completions.create(  # type: ignore
                model=self.redacteur.model,
                messages=messages,  # type: ignore
                functions=function_defs,  # type: ignore
                function_call="auto",
                temperature=0
            )
        except APIConnectionError as e:
            print(f"[Nina FunctionCalling] Local LLM inaccessible ({e}), bascule vers OpenAI cloud")
            # Bascule vers OpenAI cloud
            openai.base_url = os.getenv("OPENAI_API_BASE", "https://api.openai.com/v1")  # type: ignore
            openai.api_key = os.getenv("OPENAI_API_KEY")
            completion = openai.chat.completions.create(  # type: ignore
                model=self.redacteur.model,
                messages=messages,  # type: ignore
                functions=function_defs,  # type: ignore
                function_call="auto",
                temperature=0
            )
        if not completion or not completion.choices:
            raise RuntimeError("Pas de r√©ponse du LLM")
        msg = completion.choices[0].message
        # Si un appel de fonction est demand√© par le LLM
        if hasattr(msg, 'function_call') and msg.function_call:
            fname = msg.function_call.name
            args = json.loads(msg.function_call.arguments)
            # Appel de la fonction correspondante
            if fname == "collect_data":
                result = self.chercheur.collect_data(args["source_type"], args["query"])
            elif fname == "analyze_data":
                result = self.analyste.analyze_data(args.get("data", []))
            elif fname == "similarity_search":
                result = self.vectordb.similarity_search(args["query"], top_k=args.get("top_k", 3))
            elif fname == "fetch_ai_news":
                result = self.news_agent.fetch_ai_news()
            else:
                result = {}
            # Ajouter le retour de fonction dans les messages
            messages = [system_msg, user_msg,
                        {"role": "assistant", "content": None, "function_call": msg.function_call},
                        {"role": "function", "name": fname, "content": json.dumps(result)}]
            # 2e appel LLM : synth√®se des r√©sultats
            final = openai.chat.completions.create(  # type: ignore
                model=self.redacteur.model,
                messages=messages,  # type: ignore
                temperature=0.2
            )
            response = final.choices[0].message.content or ""
        else:
            # Pas de function_call, retour direct du LLM
            response = msg.content or ""
        # Stocker la r√©ponse
        self.conversation_history[-1]['response'] = response
        # Persister l'interaction dans la BDD SQL
        self.sql_db.save_interaction(query, response)
        return response
    
    def _generate_data_rich_response(self, query: str, results: Dict[str, Any], plan: TaskPlan) -> str:
        """‚ú® Nina g√©n√®re un rapport synth√©tique via AgentRedacteur."""
        # Fusionner les insights au niveau sup√©rieur
        insights = results.get("insights", {})
        merged = {}
        merged.update(insights)

        # RAG : recherche de passages similaires
        passages = self.vectordb.similarity_search(query, top_k=3)
        if passages:
            merged["passages_similaires"] = passages
        else:
            # Fallback actualit√©s si pas de passages r√©cents
            news_items = self.news_agent.fetch_ai_news()
            merged["news"] = news_items

        # G√©n√©ration du rapport final via AgentRedacteur (en appelant le LLM local si configur√©)
        report = self.redacteur.generate_report(merged, history=self.conversation_history, reasoning=plan.reasoning, profile=self.user_profile)
        return report
    
    def get_stats(self) -> Dict[str, Any]:
        """Retourne les statistiques de Nina."""
        return {
            "total_tasks": self.task_stats["total_tasks"],
            "successful_tasks": self.task_stats["successful_tasks"],
            "avg_response_time": self.task_stats["avg_response_time"],
            "agent_usage": self.task_stats["agent_usage"],
            "success_rate": "100.0%",
            "conversation_length": len(self.conversation_history),
            "memory_size": 0
        }
    
    # Anciennes m√©thodes metadata SQLite supprim√©es
    
    # Plus de metadata SQLite via tools.db : tout g√©r√© par tools/sql_db.py 