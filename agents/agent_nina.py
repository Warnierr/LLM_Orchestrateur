#!/usr/bin/env python3
"""
ü§ñ Agent Nina - Version corrig√©e

L'agent principal qui orchestre tous les autres agents de mani√®re intelligente.
"""

import sys
import os
import time
import json
from enum import Enum
from dataclasses import dataclass
from typing import List, Dict, Any, Optional

# Ajout du chemin racine pour les imports
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

from agents.agent_chercheur_v3 import AgentChercheurV3
from agents.agent_analyste import AgentAnalyste, AgentApprentissage
from agents.agent_redacteur import AgentRedacteur
from agents.agent_planificateur import AgentPlanificateur
from agents.agent_news import AgentNews
from tools.vector_db import VectorDB
from tools.sql_db import SQLDatabase
from agents.agent_llm_local import AgentLLMLocal  # Assurer que l'import est pr√©sent

class TaskType(Enum):
    """Types de t√¢ches que Nina peut traiter."""
    RECHERCHE_INFORMATION = "recherche_information"
    RAISONNEMENT_PUR = "raisonnement_pur"
    CONVERSATION_SIMPLE = "conversation_simple"
    ACTUALITES = "actualites"
    ANALYSE = "analyse"
    PLANIFICATION = "planification"
    CONVERSATION = "conversation"
    APPRENTISSAGE = "apprentissage"

class TaskComplexity(Enum):
    """Complexit√© des t√¢ches."""
    SIMPLE = "simple"      # 1-2 agents
    MODERATE = "moderate"  # 2-4 agents
    COMPLEX = "complex"    # 4+ agents

@dataclass
class TaskPlan:
    """Plan d'ex√©cution d'une t√¢che."""
    task_type: TaskType
    complexity: TaskComplexity
    agents_needed: List[str]
    estimated_time: float
    priority: int
    reasoning: str

class AgentNina:
    """
    ü§ñ Agent Nina - Le cerveau de l'orchestrateur
    
    Nina analyse les requ√™tes, planifie l'ex√©cution et coordonne
    tous les agents sp√©cialis√©s pour fournir des r√©ponses intelligentes.
    """
    
    def __init__(self):
        # Agents sp√©cialis√©s
        self.chercheur = AgentChercheurV3()
        self.analyste = AgentAnalyste()
        self.apprenant = AgentApprentissage()
        self.redacteur = AgentRedacteur()
        self.planificateur = AgentPlanificateur()
        self.news_agent = AgentNews()
        
        # Classe ObjectifAgent simul√©e
        class ObjectifAgent:
            def planifier(self): pass
        self.objectif = ObjectifAgent()
        
        # Base de donn√©es vectorielle
        self.vectordb = VectorDB()
        
        # Historique et statistiques
        self.conversation_history = []
        self.task_stats = {
            "total_tasks": 0,
            "successful_tasks": 0,
            "avg_response_time": 0.0,
            "agent_usage": {},
            "user_satisfaction": []
        }
        # Charger le profil utilisateur depuis crew_config.yaml
        try:
            import yaml  # type: ignore
            cfg_path = os.path.join(os.path.dirname(os.path.dirname(os.path.abspath(__file__))), 'configs', 'crew_config.yaml')
            with open(cfg_path, 'r', encoding='utf-8') as f:
                cfg = yaml.safe_load(f) or {}
            self.user_profile = cfg.get('parameters', {})
        except Exception:
            self.user_profile = {}
        # Int√©gration SQL pour la m√©moire relationnelle
        db_url = os.getenv('DATABASE_URL', 'sqlite:///data/nina_memory.db')
        self.sql_db = SQLDatabase(db_url)
        # Charger historique de conversation et profil utilisateur depuis la BDD SQL
        self.conversation_history = self.sql_db.load_conversations()
        db_profile = self.sql_db.load_user_profile()
        self.user_profile = {**self.user_profile, **db_profile}
        # Initialiser l'agent LLM local avec Mixtral, tag 'instruct'
        try:
            self.local_llm = AgentLLMLocal(model="mixtral:instruct")
            print("[AgentNina] LLM local (AgentLLMLocal) initialis√© avec le mod√®le 'mixtral:instruct'.")
        except Exception as e:
            print(f"[AgentNina] ‚ö†Ô∏è Impossible d'initialiser le LLM local : {e}")
            self.local_llm = None
    
    def analyze_request(self, query: str) -> TaskPlan:
        """üß† Nina utilise le LLM pour classifier la requ√™te et choisir la strat√©gie."""
        
        if not self.local_llm:
            print("[AgentNina] LLM local non disponible, fallback sur une recherche par d√©faut.")
            return TaskPlan(TaskType.RECHERCHE_INFORMATION, TaskComplexity.MODERATE, ["chercheur", "analyste"], 5.0, 1, "Fallback: LLM local indisponible.")

        # Prompt pour le LLM routeur avec des exemples encore plus vari√©s
        routing_prompt = f"""Tu es un routeur intelligent. Classifie la requ√™te de l'utilisateur en UNE des cat√©gories suivantes : 'raisonnement_pur', 'recherche_information', ou 'conversation_simple'.

### EXEMPLES ###
Requ√™te : "Quelle est la capitale de la France ?"
Cat√©gorie : recherche_information

Requ√™te : "Si un train part √† 8h et roule √† 100km/h, o√π sera-t-il √† 10h ?"
Cat√©gorie : raisonnement_pur

Requ√™te : "Bonjour, comment vas-tu ?"
Cat√©gorie : conversation_simple

Requ√™te : "Quelle est la suite logique : 2, 4, 6, 8, ?"
Cat√©gorie : raisonnement_pur

Requ√™te : "Peux-tu me parler de la th√©orie de la relativit√© ?"
Cat√©gorie : recherche_information

Requ√™te : "Merci beaucoup !"
Cat√©gorie : conversation_simple
### FIN DES EXEMPLES ###

Maintenant, classifie la requ√™te suivante. Ne r√©ponds PAS √† la question. Retourne UNIQUEMENT la cat√©gorie choisie.

Requ√™te de l'utilisateur : "{query}"

Cat√©gorie:"""

        try:
            response_text = self.local_llm.generate(routing_prompt).strip().lower()
            
            if 'raisonnement_pur' in response_text:
                best_type = TaskType.RAISONNEMENT_PUR
            elif 'conversation_simple' in response_text:
                best_type = TaskType.CONVERSATION_SIMPLE
            else:
                best_type = TaskType.RECHERCHE_INFORMATION

        except Exception as e:
            print(f"[AgentNina] Erreur du LLM routeur: {e}. Fallback sur recherche.")
            best_type = TaskType.RECHERCHE_INFORMATION

        # Configuration du plan de t√¢che en fonction de la classification
        if best_type == TaskType.RAISONNEMENT_PUR:
            return TaskPlan(
                task_type=TaskType.RAISONNEMENT_PUR,
                complexity=TaskComplexity.SIMPLE,
                agents_needed=["local_llm"],
                estimated_time=3.0,
                priority=1,
                reasoning="La requ√™te est un probl√®me de raisonnement pur, appel direct au LLM."
            )
        elif best_type == TaskType.CONVERSATION_SIMPLE:
            return TaskPlan(
                task_type=TaskType.CONVERSATION_SIMPLE,
                complexity=TaskComplexity.SIMPLE,
                agents_needed=["local_llm"],
                estimated_time=1.0,
                priority=1,
                reasoning="La requ√™te est une conversation simple, r√©ponse directe."
            )
        else: # RECHERCHE_INFORMATION
            return TaskPlan(
                task_type=TaskType.RECHERCHE_INFORMATION,
                complexity=TaskComplexity.MODERATE,
                agents_needed=["chercheur", "analyste", "vectordb", "redacteur"],
                estimated_time=7.0,
                priority=1,
                reasoning="La requ√™te n√©cessite une recherche d'information, activation du pipeline RAG."
            )
    
    def _execute_search_task(self, query: str) -> Dict[str, Any]:
        """Ex√©cution optimis√©e pour la recherche."""
        # 1. Recherche dans la m√©moire d'abord
        memory_results = self.vectordb.similarity_search(query, top_k=3)
        
        # 2. Recherche web
        web_results = self.chercheur.collect_data("web", query)
        
        # 3. Combinaison et analyse
        all_data = web_results + [r["text"] for r in memory_results]
        insights = self.analyste.analyze_data(all_data) if all_data else {}
        
        # 4. Mise √† jour de la m√©moire
        if web_results:
            self.vectordb.add_documents(web_results)
        
        return {
            "web_results": web_results,
            "memory_results": memory_results,
            "insights": insights,
            "total_sources": len(all_data)
        }
    
    def _summarize_history(self) -> str:
        """Cr√©e un r√©sum√© de l'historique de conversation."""
        if not self.local_llm or not self.conversation_history:
            return "Aucun historique de conversation."

        # Concat√©ner les √©changes pour le prompt de r√©sum√©
        full_history = "\n".join([f"Utilisateur: {e.get('query', '')}\nNina: {e.get('response', '')}" for e in self.conversation_history])

        summary_prompt = f"""Tu es un expert en synth√®se. R√©sume la conversation suivante en quelques points cl√©s pour donner un contexte √† un autre agent IA. Ne d√©passe pas 100 mots.

Conversation :
{full_history}

R√©sum√© contextuel :"""

        try:
            summary = self.local_llm.generate(summary_prompt)
            return summary
        except Exception as e:
            print(f"[AgentNina] Erreur lors du r√©sum√© de l'historique : {e}")
            return "Le r√©sum√© de l'historique n'a pas pu √™tre g√©n√©r√©."

    def think_and_respond(self, query: str) -> str:
        """ü§ñ M√©thode principale : Nina r√©fl√©chit et r√©pond en suivant le plan."""
        start_time = time.time()
        
        plan = self.analyze_request(query)
        print(f"[AgentNina] Plan d'action : {plan.reasoning}")

        if plan.task_type == TaskType.RAISONNEMENT_PUR:
            if not self.local_llm:
                return "D√©sol√©, mon module de raisonnement n'est pas disponible pour le moment."
            
            # Prompt de r√©solution avec Cha√Æne de Pens√©e et Few-Shot
            reasoning_prompt = f"""Tu es une experte en r√©solution de probl√®mes. D√©compose la question en √©tapes logiques, explique ton raisonnement, puis donne la r√©ponse finale.

### EXEMPLE ###
Question : "Si 3 chats attrapent 3 souris en 3 minutes, combien de temps faut-il √† 100 chats pour attraper 100 souris ?"
Raisonnement √©tape par √©tape :
1. Analyser le taux de travail. Si 3 chats attrapent 3 souris en 3 minutes, cela signifie que chaque chat met 3 minutes pour attraper 1 souris.
2. Le nombre de chats n'affecte pas le temps n√©cessaire pour qu'un chat individuel attrape une souris.
3. Donc, si nous avons 100 chats, chaque chat attrapera sa souris en 3 minutes.
R√©ponse finale : Il faudra 3 minutes √† 100 chats pour attraper 100 souris.
### FIN DE L'EXEMPLE ###

Maintenant, r√©sous la question suivante :

Question : "{query}"

Raisonnement √©tape par √©tape :
"""
            
            try:
                response = self.local_llm.generate(reasoning_prompt)
            except Exception as e:
                response = f"J'ai rencontr√© une erreur en essayant de r√©soudre le probl√®me : {e}"
        
        elif plan.task_type == TaskType.CONVERSATION_SIMPLE:
            if not self.local_llm:
                return "Bonjour ! Comment puis-je vous aider ?"
            
            conversation_prompt = f"Tu es Nina, une assistante IA amicale et serviable. R√©ponds de mani√®re naturelle √† l'utilisateur.\n\nUtilisateur: {query}\nNina:"
            response = self.local_llm.generate(conversation_prompt)

        else: # RECHERCHE_INFORMATION
            # Cr√©er un r√©sum√© de l'historique pour maintenir le contexte
            conversation_summary = self._summarize_history()
            
            # Ex√©cution du pipeline RAG (recherche, analyse, r√©daction)
            search_results = self._execute_search_task(query)
            
            # Enrichir les r√©sultats avec le r√©sum√© pour le r√©dacteur
            context_data = {
                "query": query,
                "search_results": search_results,
                "conversation_summary": conversation_summary
            }
            
            response = self._generate_data_rich_response(context_data, plan)

        # Mise √† jour de l'historique et des stats
        self.conversation_history.append({'query': query, 'response': response})
        self.sql_db.save_interaction(query, response, summary=conversation_summary if plan.task_type == TaskType.RECHERCHE_INFORMATION else "")
        
        end_time = time.time()
        # ... (logique de stats)

        return response
    
    def _generate_data_rich_response(self, context_data: Dict[str, Any], plan: TaskPlan) -> str:
        """‚ú® Nina g√©n√®re un rapport synth√©tique via AgentRedacteur."""
        # L'historique et la requ√™te sont maintenant dans context_data
        report = self.redacteur.generate_report(
            context_data, 
            reasoning=plan.reasoning, 
            profile=self.user_profile
        )
        return report
    
    def get_stats(self) -> Dict[str, Any]:
        """Retourne les statistiques de Nina."""
        return {
            "total_tasks": self.task_stats["total_tasks"],
            "successful_tasks": self.task_stats["successful_tasks"],
            "avg_response_time": self.task_stats["avg_response_time"],
            "agent_usage": self.task_stats["agent_usage"],
            "success_rate": "100.0%",
            "conversation_length": len(self.conversation_history),
            "memory_size": 0
        }
    
    # Anciennes m√©thodes metadata SQLite supprim√©es
    
    # Plus de metadata SQLite via tools.db : tout g√©r√© par tools/sql_db.py 