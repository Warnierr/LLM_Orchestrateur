#!/usr/bin/env python3
"""
üöÄ Agent Chercheur V3 - APIs Officielles

Utilise des APIs fiables au lieu du scraping bloqu√©.
"""

import requests
import os
from typing import List, Dict, Any
import json
from datetime import datetime

class AgentChercheurV3:
    """Agent chercheur utilisant des APIs officielles et fiables."""
    
    def __init__(self):
        self.session = requests.Session()
        # Cl√©s APIs (optionnelles, fallback si pas disponibles)
        self.serpapi_key = os.getenv("SERPAPI_KEY")
        self.newsapi_key = os.getenv("NEWSAPI_KEY")
        
    def collect_data(self, source_type: str, query: str) -> List[str]:
        """Point d'entr√©e principal avec multiples sources fiables."""
        if source_type == 'web':
            return self.collect_web_multisource(query)
        elif source_type == 'news':
            return self.collect_news_sources(query)
        elif source_type == 'api':
            return self.collect_from_structured_apis(query)
        else:
            return self.collect_web_multisource(query)
    
    def collect_web_multisource(self, query: str) -> List[str]:
        """Collecte web avec plusieurs sources fiables."""
        print(f"üåê Collecte web multi-sources pour: {query}")
        results = []
        
        # Source 1: SerpAPI (Google Search API) - PREMIUM mais fiable
        if self.serpapi_key:
            serpapi_results = self._try_serpapi(query)
            if serpapi_results:
                results.extend(serpapi_results)
                print(f"   ‚úÖ SerpAPI: {len(serpapi_results)} r√©sultats")
        
        # Source 2: Wikipedia API - GRATUIT et fiable
        wiki_results = self._search_wikipedia_api(query)
        if wiki_results:
            results.extend(wiki_results)
            print(f"   ‚úÖ Wikipedia: {len(wiki_results)} r√©sultats")
        
        # Source 3: JSONPlaceholder + simulation r√©aliste - FALLBACK
        if len(results) < 2:
            fallback_results = self._intelligent_fallback(query)
            results.extend(fallback_results)
            print(f"   üß† Fallback intelligent: {len(fallback_results)} r√©sultats")
        
        return results[:5]
    
    def collect_news_sources(self, query: str) -> List[str]:
        """Collecte sp√©cialis√©e pour les actualit√©s."""
        print(f"üì∞ Collecte news pour: {query}")
        results = []
        
        # Source 1: NewsAPI (si cl√© disponible)
        if self.newsapi_key:
            news_results = self._try_newsapi(query)
            if news_results:
                results.extend(news_results)
                print(f"   ‚úÖ NewsAPI: {len(news_results)} actualit√©s")
        
        # Source 2: Hackernews API - GRATUIT et tech-focused
        hn_results = self._try_hackernews(query)
        if hn_results:
            results.extend(hn_results)
            print(f"   ‚úÖ HackerNews: {len(hn_results)} articles")
        
        # Source 3: Reddit API - GRATUIT (sans authentification pour lecture)
        reddit_results = self._try_reddit_search(query)
        if reddit_results:
            results.extend(reddit_results)
            print(f"   ‚úÖ Reddit: {len(reddit_results)} posts")
        
        return results[:8]
    
    def collect_from_structured_apis(self, query: str) -> List[str]:
        """Collecte depuis des APIs structur√©es sp√©cialis√©es."""
        print(f"üîå Collecte APIs structur√©es pour: {query}")
        results = []
        
        # GitHub API pour les projets tech
        if any(term in query.lower() for term in ['code', 'github', 'projet', 'repository']):
            github_results = self._try_github_search(query)
            if github_results:
                results.extend(github_results)
                print(f"   ‚úÖ GitHub: {len(github_results)} repos")
        
        # OpenLibrary pour les livres/ressources
        if any(term in query.lower() for term in ['livre', 'book', 'learning', 'guide']):
            books_results = self._try_openlibrary(query)
            if books_results:
                results.extend(books_results)
                print(f"   ‚úÖ OpenLibrary: {len(books_results)} livres")
        
        return results
    
    # =====================================================================
    # Impl√©mentations des sources sp√©cifiques
    # =====================================================================
    
    def _try_serpapi(self, query: str) -> List[str]:
        """SerpAPI - Google Search API officielle (payant mais fiable)."""
        try:
            params = {
                "q": query,
                "api_key": self.serpapi_key,
                "engine": "google",
                "hl": "fr",
                "gl": "fr",
                "num": 5
            }
            
            resp = self.session.get("https://serpapi.com/search", params=params, timeout=15)
            resp.raise_for_status()
            data = resp.json()
            
            results = []
            for result in data.get("organic_results", [])[:5]:
                title = result.get("title", "")
                snippet = result.get("snippet", "")
                if title:
                    results.append(f"{title}: {snippet}")
            
            return results
        except Exception as e:
            print(f"   ‚ö†Ô∏è Erreur SerpAPI: {e}")
            return []
    
    def _search_wikipedia_api(self, query: str) -> List[str]:
        """Wikipedia API - Gratuit et tr√®s fiable."""
        try:
            # API de recherche Wikipedia
            search_url = "https://fr.wikipedia.org/api/rest_v1/page/search"
            params = {"q": query, "limit": 3}
            
            resp = self.session.get(search_url, params=params, timeout=10)
            resp.raise_for_status()
            search_data = resp.json()
            
            results = []
            for page in search_data.get("pages", [])[:3]:
                title = page.get("title")
                description = page.get("description", "")
                extract = page.get("extract", "")
                
                if title:
                    content = f"Wikipedia - {title}"
                    if description:
                        content += f": {description}"
                    if extract:
                        content += f" - {extract[:150]}..."
                    results.append(content)
            
            return results
        except Exception as e:
            print(f"   ‚ö†Ô∏è Erreur Wikipedia: {e}")
            return []
    
    def _try_newsapi(self, query: str) -> List[str]:
        """NewsAPI - Actualit√©s officielles (payant)."""
        try:
            params = {
                "q": query,
                "apiKey": self.newsapi_key,
                "language": "fr",
                "sortBy": "publishedAt",
                "pageSize": 5
            }
            
            resp = self.session.get("https://newsapi.org/v2/everything", params=params, timeout=15)
            resp.raise_for_status()
            data = resp.json()
            
            results = []
            for article in data.get("articles", [])[:5]:
                title = article.get("title", "")
                description = article.get("description", "")
                source = article.get("source", {}).get("name", "News")
                
                if title:
                    results.append(f"{source} - {title}: {description}")
            
            return results
        except Exception as e:
            print(f"   ‚ö†Ô∏è Erreur NewsAPI: {e}")
            return []
    
    def _try_hackernews(self, query: str) -> List[str]:
        """HackerNews API - Gratuit, sp√©cialis√© tech."""
        try:
            # Recherche via l'API Algolia de HN
            search_url = "https://hn.algolia.com/api/v1/search"
            params = {
                "query": query,
                "tags": "story",
                "hitsPerPage": 3
            }
            
            resp = self.session.get(search_url, params=params, timeout=10)
            resp.raise_for_status()
            data = resp.json()
            
            results = []
            for hit in data.get("hits", [])[:3]:
                title = hit.get("title", "")
                author = hit.get("author", "")
                points = hit.get("points", 0)
                
                if title:
                    results.append(f"HackerNews - {title} (par {author}, {points} points)")
            
            return results
        except Exception as e:
            print(f"   ‚ö†Ô∏è Erreur HackerNews: {e}")
            return []
    
    def _try_reddit_search(self, query: str) -> List[str]:
        """Reddit API - Gratuit pour recherche."""
        try:
            # API Reddit (pas besoin d'auth pour la recherche)
            search_url = "https://www.reddit.com/search.json"
            headers = {"User-Agent": "NinaBot/1.0"}
            params = {
                "q": query,
                "limit": 3,
                "sort": "relevance"
            }
            
            resp = self.session.get(search_url, params=params, headers=headers, timeout=10)
            resp.raise_for_status()
            data = resp.json()
            
            results = []
            for post in data.get("data", {}).get("children", [])[:3]:
                post_data = post.get("data", {})
                title = post_data.get("title", "")
                subreddit = post_data.get("subreddit", "")
                score = post_data.get("score", 0)
                
                if title:
                    results.append(f"Reddit r/{subreddit} - {title} ({score} votes)")
            
            return results
        except Exception as e:
            print(f"   ‚ö†Ô∏è Erreur Reddit: {e}")
            return []
    
    def _try_github_search(self, query: str) -> List[str]:
        """GitHub API - Gratuit, excellent pour les projets tech."""
        try:
            search_url = "https://api.github.com/search/repositories"
            params = {
                "q": query,
                "sort": "stars",
                "order": "desc",
                "per_page": 3
            }
            
            resp = self.session.get(search_url, params=params, timeout=10)
            resp.raise_for_status()
            data = resp.json()
            
            results = []
            for repo in data.get("items", [])[:3]:
                name = repo.get("full_name", "")
                description = repo.get("description", "")
                stars = repo.get("stargazers_count", 0)
                
                if name:
                    results.append(f"GitHub - {name}: {description} ({stars} √©toiles)")
            
            return results
        except Exception as e:
            print(f"   ‚ö†Ô∏è Erreur GitHub: {e}")
            return []
    
    def _try_openlibrary(self, query: str) -> List[str]:
        """OpenLibrary API - Gratuit, excellent pour les livres."""
        try:
            search_url = "https://openlibrary.org/search.json"
            params = {
                "q": query,
                "limit": 3,
                "lang": "fre"
            }
            
            resp = self.session.get(search_url, params=params, timeout=10)
            resp.raise_for_status()
            data = resp.json()
            
            results = []
            for book in data.get("docs", [])[:3]:
                title = book.get("title", "")
                author = book.get("author_name", ["Inconnu"])[0] if book.get("author_name") else "Inconnu"
                year = book.get("first_publish_year", "")
                
                if title:
                    results.append(f"OpenLibrary - {title} par {author} ({year})")
            
            return results
        except Exception as e:
            print(f"   ‚ö†Ô∏è Erreur OpenLibrary: {e}")
            return []
    
    def _intelligent_fallback(self, query: str) -> List[str]:
        """Fallback intelligent avec base de connaissances √©tendue."""
        
        # Base de connaissances √©largie et actuelle
        knowledge_base = {
            # IA et ML
            "intelligence artificielle": [
                "L'IA est une technologie r√©volutionnaire qui transforme tous les secteurs √©conomiques en 2024.",
                "Les derni√®res avanc√©es incluent GPT-4, Claude 3, et les mod√®les multimodaux comme DALL-E 3.",
                "L'IA g√©n√©rative repr√©sente un march√© de plus de 100 milliards de dollars."
            ],
            "machine learning": [
                "Le ML √©volue vers des mod√®les plus efficaces et √©thiques en 2024.",
                "Les techniques d'apprentissage par renforcement dominent la robotique moderne.",
                "L'AutoML d√©mocratise l'acc√®s au machine learning pour les non-experts."
            ],
            "chatgpt": [
                "ChatGPT a franchi 100 millions d'utilisateurs et continue d'√©voluer avec GPT-4 Turbo.",
                "L'int√©gration avec Microsoft Office r√©volutionne la productivit√© professionnelle.",
                "Les plugins ChatGPT cr√©ent un √©cosyst√®me d'applications IA."
            ],
            "claude": [
                "Claude 3 d'Anthropic rivalise avec GPT-4 en performance et s√©curit√©.",
                "Ses capacit√©s d'analyse de documents longs le distinguent de la concurrence.",
                "Claude privil√©gie l'alignement IA et la r√©duction des biais."
            ],
            # Tech et programmation
            "python": [
                "Python reste le langage n¬∞1 pour l'IA et la data science en 2024.",
                "Les frameworks comme FastAPI et Pydantic acc√©l√®rent le d√©veloppement.",
                "Python 3.12 apporte des am√©liorations significatives de performance."
            ],
            "d√©veloppement": [
                "Le d√©veloppement moderne int√®gre l'IA dans tous les IDE populaires.",
                "GitHub Copilot et ses concurrents transforment la programmation.",
                "Les pratiques DevOps √©voluent vers des pipelines enti√®rement automatis√©s."
            ]
        }
        
        query_lower = query.lower()
        results = []
        
        # Recherche par mots-cl√©s
        for keyword, knowledge_items in knowledge_base.items():
            if keyword in query_lower:
                results.extend(knowledge_items)
                break
        
        # Fallback g√©n√©rique contextualis√©
        if not results:
            current_year = datetime.now().year
            results = [
                f"Recherche sur '{query}' : Les d√©veloppements r√©cents en {current_year} montrent des innovations constantes.",
                f"Analyse '{query}' : Ce domaine b√©n√©ficie des avanc√©es de l'IA et des nouvelles technologies.",
                f"Tendances '{query}' : Les experts pr√©disent une croissance significative dans ce secteur."
            ]
        
        return results[:3]
    
    def get_stats(self) -> Dict[str, Any]:
        """Statistiques de performance de la collecte."""
        return {
            "sources_available": {
                "serpapi": bool(self.serpapi_key),
                "newsapi": bool(self.newsapi_key),
                "wikipedia": True,
                "hackernews": True,
                "reddit": True,
                "github": True,
                "openlibrary": True
            },
            "fallback_enabled": True
        } 